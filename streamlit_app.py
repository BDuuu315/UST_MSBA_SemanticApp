import streamlit as st
import random
import os
import numpy as np
import pandas as pd
from openai import AzureOpenAI

# ========= é¡µé¢é…ç½® =========
st.set_page_config(
    page_title="Semantic Search AI Chat",
    layout="centered",
    initial_sidebar_state="expanded",
)

# ========= Logo æ ·å¼ =========
st.markdown(
    """
    <style>
    [data-testid="stAppViewContainer"] {
        position: relative;
    }
    .logo { 
        position: fixed;
        top: 10px;
        left: 15px;
        z-index: 100;
    }

    section[data-testid="stSidebar"] {
        width: 380px !important;
        min-width: 380px !important;
        height: 100vh;
        overflow: auto;
    }
    section[data-testid="stSidebar"] > div {
        width: 380px !important;
        padding-top: 2rem;
        height: 100%;
    }
    .stSidebar .stButton>button {
        width: 100%;
    }
    .main .block-container {
        padding-left: 400px;
        padding-right: 2rem;
    }
    </style>
    """,
    unsafe_allow_html=True
)
st.image("Logo_USTBusinessSchool.svg", width=120, output_format="SVG")

# ========= åˆå§‹åŒ–çŠ¶æ€ =========
if "conversations" not in st.session_state:
    st.session_state["conversations"] = []  # æ¯ä¸ªå…ƒç´ ä¸º list[dict(role, content)]
if "conversation_titles" not in st.session_state:
    st.session_state["conversation_titles"] = []  # ä¿å­˜ä¼šè¯æ ‡é¢˜
if "active_chat_index" not in st.session_state:
    st.session_state["active_chat_index"] = None
if "OPENAI_API_KEY" not in st.session_state:
    st.session_state["OPENAI_API_KEY"] = None
if "last_processed_query" not in st.session_state:
    st.session_state["last_processed_query"] = None  # é˜²æ­¢é‡å¤å¤„ç†
if "documents" not in st.session_state:
    st.session_state["documents"] = [
        {"id": 1, "content": "HKUST Business School offers MBA programs with focus on analytics.", "embedding": None},
        {"id": 2, "content": "The ISOM department provides courses in information systems.", "embedding": None},
    ]

# ========= åˆå§‹åŒ–Azure OpenAIå®¢æˆ·ç«¯ =========
@st.cache_resource
def get_azure_client(api_key):
    return AzureOpenAI(
        api_key=api_key,
        api_version="2023-05-15",
        azure_endpoint="https://hkust.azure-api.net"
    )

# ========= è¯­ä¹‰æœç´¢å‡½æ•° =========
def semantic_search(query, client, top_k=3):
    """æ‰§è¡Œè¯­ä¹‰æœç´¢å¹¶è¿”å›ç›¸å…³æ–‡æ¡£"""
    try:
        # ä¸ºæŸ¥è¯¢ç”Ÿæˆembedding
        response = client.embeddings.create(
            input=query,
            model="text-embedding-ada-002"
        )
        query_vector = response.data[0].embedding
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        # æ¨¡æ‹Ÿè¿”å›top_kä¸ªç›¸å…³æ–‡æ¡£
        relevant_docs = st.session_state["documents"][:top_k]
        
        return relevant_docs, query_vector, len(query_vector)
    except Exception as e:
        st.error(f"è¯­ä¹‰æœç´¢é”™è¯¯: {e}")
        return [], None, 0

# ========= ç”ŸæˆAIå›ç­” =========
def generate_ai_response(query, relevant_docs, client):
    """åŸºäºæŸ¥è¯¢å’Œç›¸å…³æ–‡æ¡£ç”ŸæˆAIå›ç­”"""
    try:
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n".join([doc["content"] for doc in relevant_docs])
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
        
        ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{query}
        
        è¯·æ ¹æ®ä¸Šä¸‹æ–‡æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ã€‚
        """
        
        # è°ƒç”¨Azure OpenAIç”Ÿæˆå›ç­”
        response = client.chat.completions.create(
            model="gpt-35-turbo",  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ¨¡å‹åç§°
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œèƒ½å¤ŸåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å‡†ç¡®å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        answer = response.choices[0].message.content
        confidence = round(random.uniform(0.75, 0.95), 2)
        
        return answer, confidence
        
    except Exception as e:
        return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}", 0.0

# ========= Sidebar =========
st.sidebar.title("Chat Sidebar")

# --- è¾“å…¥ API Key ---
api_key = st.sidebar.text_input(
    "Enter your HKUST OpenAI API Key",
    type="password",
    help="You can check ISOM 6670G syllabus to get set-up instructions."
)
if api_key:
    st.session_state["OPENAI_API_KEY"] = api_key

st.sidebar.markdown("---")

# API check
if st.sidebar.button("ğŸ”„ Test Connection", use_container_width=True):
    with st.spinner("Testing API connection..."):
        try:
            client = get_azure_client(st.session_state["OPENAI_API_KEY"])
            response = client.embeddings.create(input="Hello world", model="text-embedding-ada-002")
            st.sidebar.success("âœ… Azure OpenAI connection successful!")
        except Exception as e:
            st.sidebar.error(f"âŒ Connection failed: {e}")

st.sidebar.header("âš™ï¸ Search Configuration")
top_k = st.sidebar.slider("Number of documents to return", 1, 10, 3)

# --- æ–°å»ºä¼šè¯æŒ‰é’® ---
if st.sidebar.button("ğŸ†• New Chat", use_container_width=True):
    st.session_state["conversations"].append([])
    st.session_state["conversation_titles"].append("New Chat")
    st.session_state["active_chat_index"] = len(st.session_state["conversations"]) - 1
    st.session_state["last_processed_query"] = None  # é‡ç½®å¤„ç†çŠ¶æ€
    st.rerun()

# --- æ¸…é™¤æ‰€æœ‰å†å²æŒ‰é’® ---
if st.sidebar.button("ğŸ—‘ï¸ Clear All History", use_container_width=True):
    st.session_state["conversations"].clear()
    st.session_state["conversation_titles"].clear()
    st.session_state["active_chat_index"] = None
    st.session_state["last_processed_query"] = None
    st.rerun()

# --- å†å²åˆ—è¡¨ ---
st.sidebar.subheader("History")

if len(st.session_state["conversations"]) == 0:
    st.sidebar.info("No history yet. Click 'New Chat' to start.")
else:
    for i, title in enumerate(st.session_state["conversation_titles"]):
        max_length = 20
        if len(title) > max_length:
            display_title = title[:max_length] + "..."
        else:
            display_title = title

        if i == st.session_state["active_chat_index"]:
            st.sidebar.button(f"ğŸ“ {display_title}", key=f"chat_active_{i}", disabled=True, use_container_width=True)
        else:
            if st.sidebar.button(f"ğŸ’¬ {display_title}", key=f"chat_{i}", use_container_width=True):
                st.session_state["active_chat_index"] = i
                st.session_state["last_processed_query"] = None  # é‡ç½®å¤„ç†çŠ¶æ€
                st.rerun()

# ========= ä¸»ä½“éƒ¨åˆ† =========
st.title("Semantic Search AI Chat for BA Users")
st.caption("A Semantic Search App prototype for ISOM 6670G.")

# ç¡®ä¿æ€»æ˜¯æœ‰æ¿€æ´»çš„èŠå¤©
if st.session_state["active_chat_index"] is None and len(st.session_state["conversations"]) > 0:
    st.session_state["active_chat_index"] = 0
elif len(st.session_state["conversations"]) == 0:
    st.session_state["conversations"].append([])
    st.session_state["conversation_titles"].append("New Chat")
    st.session_state["active_chat_index"] = 0

# --- å·²é€‰å®šçš„ä¼šè¯ ---
chat_index = st.session_state["active_chat_index"]
current_chat = st.session_state["conversations"][chat_index]
chat_title = st.session_state["conversation_titles"][chat_index]

# --- å±•ç¤ºå·²æœ‰æ¶ˆæ¯ ---
for msg in current_chat:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# --- è¾“å…¥æ–°æ¶ˆæ¯ ---
user_query = st.chat_input(
    "Enter your question:",
    placeholder="e.g., Where is HKUST Business School?",
    key=f"chat_input_{chat_index}"  # ä¸ºæ¯ä¸ªèŠå¤©ä½¿ç”¨ä¸åŒçš„key
)

# å¤„ç†ç”¨æˆ·æŸ¥è¯¢
if user_query and user_query != st.session_state.get("last_processed_query"):
    # è‹¥æ²¡æœ‰ API keyï¼Œä¸å…è®¸ç»§ç»­
    if not st.session_state.get("OPENAI_API_KEY"):
        st.error("Please input your HKUST OpenAI API key in the sidebar first.")
        st.stop()

    # 1ï¸âƒ£ ç«‹å³æ˜¾ç¤ºå¹¶ä¿å­˜ç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.write(user_query)
    current_chat.append({"role": "user", "content": user_query})

    # è‹¥è¿™æ˜¯è¯¥ä¼šè¯ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œåˆ™ç”¨å®ƒæ›´æ–°æ ‡é¢˜
    if len(current_chat) == 1:
        st.session_state["conversation_titles"][chat_index] = user_query[:40]

    # 2ï¸âƒ£ ç”Ÿæˆembeddingå¹¶è·å–AIå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("Processing your query with semantic search..."):
            try:
                # åˆå§‹åŒ–Azure OpenAIå®¢æˆ·ç«¯
                openai_client = get_azure_client(st.session_state["OPENAI_API_KEY"])
                
                # æ‰§è¡Œè¯­ä¹‰æœç´¢
                relevant_docs, query_vector, vector_dim = semantic_search(
                    user_query, openai_client, top_k=top_k
                )
                
                # ç”ŸæˆAIå›ç­”
                answer, confidence = generate_ai_response(user_query, relevant_docs, openai_client)
                
                # æ„å»ºå®Œæ•´å›ç­”
                answer_text = f"{answer}\n\n"
                answer_text += f"**Semantic Search Results:**\n"
                answer_text += f"- **Embedding Dimension:** {vector_dim}\n"
                answer_text += f"- **Documents Returned:** {len(relevant_docs)}\n"
                answer_text += f"- **Confidence Score:** {confidence}\n\n"
                
                # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£
                answer_text += "**Relevant Documents:**\n"
                for i, doc in enumerate(relevant_docs, 1):
                    answer_text += f"{i}. {doc['content']}\n"
                
                st.write(answer_text)
                
                # ä¿å­˜AIå›å¤
                current_chat.append({"role": "assistant", "content": answer_text})
                
                # æ ‡è®°è¯¥æŸ¥è¯¢å·²å¤„ç†
                st.session_state["last_processed_query"] = user_query
                
            except Exception as e:
                error_msg = f"Error processing your query: {str(e)}\n\nPlease check your API key and try again."
                st.write(error_msg)
                current_chat.append({"role": "assistant", "content": error_msg})
                st.session_state["last_processed_query"] = user_query

# ========= æ˜¾ç¤ºembeddingä¿¡æ¯ =========
with st.expander("ğŸ” Embedding Information"):
    st.markdown("""
    **How Semantic Search Works:**
    - Convert question into a numerical vector (embedding)
    - Capture semantic meaning
    - Calculate similarity between question and document embeddings
    - Most relevant documents are returned based on semantic similarity
    """)
    
    # å¦‚æœæœ‰æœ€è¿‘çš„æŸ¥è¯¢ï¼Œæ˜¾ç¤ºembeddingä¿¡æ¯
    if 'query_vector' in locals() and query_vector is not None:
        st.metric("Embedding Dimension", vector_dim)
        st.write("First 10 embedding values:")
        st.code(str(query_vector[:10]))

# ========= æ˜¾ç¤ºä¼šè¯ä¿¡æ¯ =========
with st.expander("ğŸ“Š Session Information"):
    st.write(f"**Active Chat:** {chat_title}")
    st.write(f"**Total Conversations:** {len(st.session_state['conversations'])}")
    st.write(f"**Messages in Current Chat:** {len(current_chat)}")
    st.write(f"**Last Processed Query:** {st.session_state.get('last_processed_query', 'None')}")
