# =============================================================
# RAG System with Azure OpenAI + Pinecone (integrated version)
# =============================================================

from openai import AzureOpenAI
from pinecone import Pinecone
import numpy as np
import os
from datetime import datetime
from typing import Dict, Any

# =============================================================
# 1ï¸âƒ£ Azure OpenAI åˆå§‹åŒ–
# =============================================================
openai_client = AzureOpenAI(
    api_key="aed78ad4701e4823ad0e7e233c877b8c",   # âš ï¸ è¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ API KEY
    api_version="2023-05-15",
    azure_endpoint="https://hkust.azure-api.net"
)

# =============================================================
# 2ï¸âƒ£ æ›¿æ¢åçš„ Pinecone åˆå§‹åŒ–ä¸è¯­ä¹‰æ£€ç´¢é€»è¾‘
# =============================================================

def get_pinecone_client():
    """
    åˆå§‹åŒ– Pinecone å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ Streamlit ç‰ˆæœ¬é…ç½®ï¼‰
    """
    pc = Pinecone(api_key="pcsk_JPQMS_zQZ9MfrD4aSEe8b69PoxsjcsvoSPEHpzgYGt4GPm8bv7ED95Wjy4u7vPmxSnjj")
    index = pc.Index(
        name="developer-quickstart-py",
        host="https://developer-quickstart-py-9d1pu2j.svc.aped-4627-b74a.pinecone.io"
    )
    return index


def semantic_search(user_query: str, openai_client, top_k: int = 5):
    """
    è¯­ä¹‰æ£€ç´¢ï¼šä½¿ç”¨ Azure ç”Ÿæˆ embedding -> Pinecone æœç´¢ç›¸ä¼¼æ–‡æ¡£
    """
    # === Step 1. ç”Ÿæˆ query å‘é‡ ===
    emb = openai_client.embeddings.create(
        input=user_query,
        model="text-embedding-ada-002"
    )
    query_vector = emb.data[0].embedding

    # === Step 2. æ£€ç´¢ Pinecone ===
    index = get_pinecone_client()
    search_resp = index.query(
        vector=query_vector,
        top_k=top_k,
        include_metadata=True
    )

    # === Step 3. è¾“å‡ºæ£€ç´¢æ—¥å¿— ===
    print(f"\nQuery: {user_query}\n")
    print("-" * 60)
    for i, match in enumerate(search_resp.matches, 1):
        text = match.metadata.get("text", "[no text]")
        print(f"[{i}] Score: {match.score:.4f} | {text[:120]}{'...' if len(text)>120 else ''}")

    return query_vector, search_resp


# =============================================================
# 3ï¸âƒ£ æ„å»ºå¢å¼º Promptï¼ˆRAGï¼‰
# =============================================================

def build_augmented_prompt(user_query: str, search_results) -> str:
    """
    ç»“åˆç”¨æˆ·é—®é¢˜ä¸ Pinecone æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæ„å»º RAG Prompt
    """
    context_chunks = []

    for i, match in enumerate(search_results.matches, 1):
        doc_text = (
            match.metadata.get("text")
            or match.metadata.get("chunk_text", "")
        )
        context_chunks.append(f"[Document {i}]\n{doc_text}")

    context_block = "\n\n".join(context_chunks)

    augmented_prompt = f"""
You are an intelligent assistant. Please answer the user's question
strictly based on the context provided below.

Guidelines:
1. Only use the information from the **Context** section.
2. Do NOT fabricate or guess.
3. If the answer is not present in the context, reply with:
   "The provided context does not contain the answer."

User Query:
{user_query}

Context:
{context_block}
""".strip()

    return augmented_prompt


# =============================================================
# 4ï¸âƒ£ RAG ä¸»æµç¨‹ï¼ˆä½¿ç”¨ Azure OpenAI å›ç­”ï¼‰
# =============================================================

def rag_answer_with_azure(
    user_question: str,
    openai_client,
    top_k: int = 5,
    model: str = "gpt-35-turbo",
    temperature: float = 0.2,
    max_tokens: int = 1536
) -> Dict[str, Any]:
    """
    ç»¼åˆ RAG æ£€ç´¢ + Azure å›ç­”
    """
    print(f"[{datetime.now().strftime('%H:%M:%S')}] User question: {user_question}")

    # æ£€ç´¢ Pinecone
    query_vec, search_results = semantic_search(user_question, openai_client, top_k=top_k)

    # æ„å»º RAG æç¤ºè¯
    aug_prompt = build_augmented_prompt(user_question, search_results)

    print("\n" + "=" * 80)
    print("Final RAG Prompt sent to LLM (preview):")
    print("=" * 80)
    print(aug_prompt[:1000] + "\n...")

    try:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Calling Azure {model}...")
        response = openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": aug_prompt}
            ],
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=0.8,
            presence_penalty=0.2,
            frequency_penalty=0.2
        )

        answer = response.choices[0].message.content.strip()
        usage = response.usage

        print(f"Token usage â†’ prompt: {usage.prompt_tokens}, completion: {usage.completion_tokens}, total: {usage.total_tokens}")

        return {
            "query": user_question,
            "answer": answer,
            "model": model,
            "usage": {
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens,
                "total_tokens": usage.total_tokens
            },
            "timestamp": datetime.now().isoformat(),
            "results": [m.metadata for m in search_results.matches]
        }

    except Exception as e:
        error_msg = f"[ChatGPT Calling Failed] {str(e)}"
        print(error_msg)
        return {
            "answer": "An error occurred while generating the response.",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


# =============================================================
# 5ï¸âƒ£ ä¸»ç¨‹åºå…¥å£ï¼ˆè°ƒè¯•ç”¨ç¤ºä¾‹ï¼‰
# =============================================================
if __name__ == "__main__":
    query = "What is disease prevention?"
    rag_result = rag_answer_with_azure(query, openai_client, top_k=5)
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ Final Answer:")
    print("=" * 80)
    print(rag_result["answer"])
