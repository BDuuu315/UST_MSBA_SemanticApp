import streamlit as st
import random
import numpy as np
import pandas as pd
import time
from openai import AzureOpenAI
from pinecone import Pinecone, ServerlessSpec

# ========= é¡µé¢é…ç½® =========
st.set_page_config(
    page_title="Semantic Search AI Chat",
    layout="centered",
    initial_sidebar_state="expanded",
)

# ========= Logo æ ·å¼ =========
st.markdown(
    """
    <style>
    [data-testid="stAppViewContainer"] {
        position: relative;
    }
    .logo { 
        position: fixed;
        top: 10px;
        left: 15px;
        z-index: 100;
    }
    section[data-testid="stSidebar"] {
        width: 380px !important;
        min-width: 380px !important;
    }
    .stSidebar .stButton>button {
        width: 100%;
    }
    </style>
    """,
    unsafe_allow_html=True
)
st.image("Logo_USTBusinessSchool.svg", width=120, output_format="SVG")


# ========= åˆå§‹åŒ–çŠ¶æ€ =========
default_docs = [
    {"id": 1, "content": "HKUST Business School offers MBA programs with focus on analytics.", "embedding": None},
    {"id": 2, "content": "The ISOM department provides courses in information systems.", "embedding": None},
]

def init_session():
    """åˆå§‹åŒ–æ‰€æœ‰ä¼šè¯çŠ¶æ€"""
    for key, default in {
        "conversations": [],
        "conversation_titles": [],
        "active_chat_index": None,
        "OPENAI_API_KEY": None,
        "documents": default_docs.copy(),
        "last_query": "",
    }.items():
        if key not in st.session_state:
            st.session_state[key] = default

init_session()


# ========= åˆå§‹åŒ–Azure OpenAIå®¢æˆ·ç«¯ =========
@st.cache_resource
def get_azure_client(api_key):
    return AzureOpenAI(
        api_key=api_key,
        api_version="2023-05-15",
        azure_endpoint="https://hkust.azure-api.net"
    )

# ========= åˆå§‹åŒ– Pinecone =========
PINECONE_API_KEY = "pcsk_JPQMS_zQZ9MfrD4aSEe8b69PoxsjcsvoSPEHpzgYGt4GPm8bv7ED95Wjy4u7vPmxSnjj"
PINECONE_INDEX_NAME = "developer-quickstart-py"
PINECONE_HOST = "https://developer-quickstart-py-9d1pu2j.svc.aped-4627-b74a.pinecone.io"

@st.cache_resource
def get_pinecone_client():
    pc = Pinecone(api_key=PINECONE_API_KEY)
    return pc.Index(name=PINECONE_INDEX_NAME, host=PINECONE_HOST)


# ========= è¾…åŠ©å‡½æ•° =========
def semantic_search(query_vector, top_k=5):
    """Pinecone æœç´¢å‡½æ•°"""
    index = get_pinecone_client()
    response = index.query(
        vector=query_vector,
        top_k=top_k,
        include_metadata=True
    )
    return response


def build_augmented_prompt(user_query: str, search_results) -> str:
    """æ„å»º RAG prompt"""
    context_list = []
    for i, match in enumerate(search_results.matches, start=1):
        context_text = match.metadata.get("text", "")
        context_list.append(f"[Document {i}]\n{context_text}")
    context_block = "\n\n".join(context_list)

    augmented_prompt = f"""
You are an intelligent assistant. Please answer the user's question
strictly based on the context provided below.

Guidelines:
1. Only use the information from the **Context** section to answer.
2. Do NOT fabricate or guess.
3. If the answer is not present in the context, reply with:
   "The provided context does not contain the answer."

User Query:
{user_query}

Context:
{context_block}
""".strip()

    return augmented_prompt


# ========= Sidebar =========
st.sidebar.title("Chat Sidebar")

api_key = st.sidebar.text_input(
    "Enter your HKUST OpenAI API Key",
    type="password",
    help="You can check ISOM 6670G syllabus to get set-up instructions."
)
if api_key:
    st.session_state["OPENAI_API_KEY"] = api_key

st.sidebar.markdown("---")

# --- API æµ‹è¯• ---
if st.sidebar.button("ğŸ”„ Test Connection", use_container_width=True):
    if not st.session_state["OPENAI_API_KEY"]:
        st.sidebar.error("Please input your API key first.")
    else:
        with st.spinner("Testing API connection..."):
            try:
                client = get_azure_client(st.session_state["OPENAI_API_KEY"])
                response = client.embeddings.create(input="Hello HKUST", model="text-embedding-ada-002")
                st.sidebar.success("âœ… Azure OpenAI connection successful!")
            except Exception as e:
                st.sidebar.error(f"âŒ Connection failed: {e}")

st.sidebar.header("ğŸ§  API Status")
col1, col2 = st.sidebar.columns(2)
col1.success("âœ… Pinecone: Connected")
col2.success("âœ… Azure OpenAI: Connected")

st.sidebar.header("âš™ï¸ Search Configuration")
top_k = st.sidebar.slider("Number of documents to return", 1, 10, 3)

st.sidebar.header("ğŸ’¡ Usage Tips")
st.sidebar.info("""
- Enter complete question statements
- More specific questions yield more accurate results
- Supports both Chinese and English queries
- System generates answers based on relevant documents
""")

st.sidebar.markdown("---")


# --- æ–°å»ºä¼šè¯ ---
if st.sidebar.button("ğŸ†• New Chat", use_container_width=True):
    st.session_state["conversations"].append([])
    st.session_state["conversation_titles"].append("New Chat")
    st.session_state["active_chat_index"] = len(st.session_state["conversations"]) - 1
    st.session_state["last_query"] = ""
    st.rerun()

# --- æ¸…ç©ºæ‰€æœ‰å†å² ---
if st.sidebar.button("ğŸ—‘ï¸ Clear All History", use_container_width=True):
    for key in ["conversations", "conversation_titles", "active_chat_index", "last_query"]:
        st.session_state[key] = None if "index" in key else []
    init_session()
    st.rerun()

# --- ä¼šè¯åˆ—è¡¨ ---
st.sidebar.subheader("History")
if not st.session_state["conversations"]:
    st.sidebar.info("No history yet. Click 'New Chat' to start.")
else:
    for i, title in enumerate(st.session_state["conversation_titles"]):
        label = f"ğŸ“ {title}" if i == st.session_state["active_chat_index"] else f"ğŸ’¬ {title}"
        if st.sidebar.button(label, key=f"chat_{i}", use_container_width=True, disabled=(i == st.session_state["active_chat_index"])):
            st.session_state["active_chat_index"] = i
            st.session_state["last_query"] = ""
            st.rerun()


# ========= ä¸»ä½“éƒ¨åˆ† =========
st.title("ğŸ” Semantic Search AI Chat for BA Users")
st.caption("A Semantic Search App prototype for ISOM 6670G.")


# --- è‹¥æ— æ¿€æ´»ä¼šè¯ ---
if st.session_state["active_chat_index"] is None:
    st.info("Click *'New Chat'* in the sidebar to start a conversation.")
    st.stop()

chat_index = st.session_state["active_chat_index"]
current_chat = st.session_state["conversations"][chat_index]


# --- å·²æœ‰æ¶ˆæ¯å±•ç¤º ---
for msg in current_chat:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])


# --- è¾“å…¥æ¡† (é¿å…è‡ªåŠ¨æäº¤é‡å¤æ‰§è¡Œ) ---
with st.form(key="query_form", clear_on_submit=True):
    user_query = st.text_input(
        "Enter your question:",
        placeholder="e.g., Where is HKUST Business School?",
    )
    submitted = st.form_submit_button("ğŸš€ Submit Query")

if submitted and user_query:
    # è‹¥æ—  key
    if not st.session_state.get("OPENAI_API_KEY"):
        st.error("Please input your HKUST OpenAI API key in the sidebar first.")
        st.stop()

    # ä¿å­˜ç”¨æˆ·è¾“å…¥
    st.session_state["last_query"] = user_query
    st.chat_message("user").write(user_query)
    current_chat.append({"role": "user", "content": user_query})

    with st.spinner("ğŸ” Searching relevant documents..."):
        try:
            openai_client = get_azure_client(st.session_state["OPENAI_API_KEY"])
            response = openai_client.embeddings.create(input=user_query, model="text-embedding-ada-002")
            query_vector = response.data[0].embedding
            vector_dim = len(query_vector)

            # Pinecone è¯­ä¹‰æœç´¢
            search_results = semantic_search(query_vector, top_k=top_k)

            # æ„å»º RAG prompt
            aug_prompt = build_augmented_prompt(user_query, search_results)

            # æ¨¡æ‹Ÿç­”æ¡ˆç”Ÿæˆ
            simulated_answer = (
                f"âœ… Processed with semantic search!\n\n"
                f"**Question:** {user_query}\n\n"
                f"**Retrieved {len(search_results.matches)} documents**\n\n"
                f"**Embedding Dimension:** {vector_dim}\n"
            )
            confidence = round(random.uniform(0.75, 0.99), 2)
            answer_text = f"{simulated_answer}\n**Confidence Score:** {confidence}"

        except Exception as e:
            answer_text = f"âŒ Error processing query: {e}"
            confidence = 0.0

    st.chat_message("assistant").write(answer_text)
    current_chat.append({"role": "assistant", "content": answer_text})


# ========= Embeddingä¿¡æ¯ =========
with st.expander("ğŸ” Embedding Information"):
    st.markdown("""
    **How Semantic Search Works:**
    - Convert question into a numerical vector (embedding)
    - Capture semantic meaning
    - Search semantically similar documents in Pinecone
    - Generate an answer based on relevant context
    """)
    if st.session_state.get("last_query"):
        st.write(f"Latest Query: {st.session_state['last_query']}")
